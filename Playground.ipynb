{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应该将这部分代码分为pretain和train两个部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri May  4 21:21:56 2018\n",
    "\n",
    "@author: shen1994\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import gensim\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from log import setUpLogger\n",
    "\n",
    "from data_create import create_label_data, path_flatten\n",
    "\n",
    "from data_preprocess import DataPreprocess\n",
    "\n",
    "from data_generate import generate_batch\n",
    "\n",
    "from bilstm_cnn_crf import bilstm_cnn_crf\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from paths import TrainPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--corpus_path\", help=\"corpus path\", default=\"/home/jovyan/shared/corpus/2014/\", type=str)\n",
    "parser.add_argument(\"--batch_size\", help=\"batch size\", default=256, type=int)\n",
    "parser.add_argument(\"--epochs\", help=\"epochs\", default=3, type=int)\n",
    "parser.add_argument(\"--use_cache_train_data\", default=False, type=bool)\n",
    "parser.add_argument(\"--max_len\", default=306, type=int)\n",
    "parser.add_argument(\"--documents_length\", default=2531574, type=int)\n",
    "parser.add_argument(\n",
    "    \"--train_dir\", help=\"train directory\", default=\"/home/jovyan/shared/\", type=str\n",
    ")\n",
    "args = parser.parse_args(args=['--use_cache_train_data=true'])\n",
    "\n",
    "corpus_path = args.corpus_path\n",
    "batch_size = args.batch_size\n",
    "epochs = args.epochs\n",
    "\n",
    "trainPath = TrainPath(args.train_dir)\n",
    "\n",
    "setUpLogger(trainPath)\n",
    "dataPreprocess = DataPreprocess(trainPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"step-1--->\" + u\"加载词向量模型\" + \"--->START\")\n",
    "embedding_model = gensim.models.Word2Vec.load(trainPath.model_vector_path)\n",
    "\n",
    "word_dict = dataPreprocess.create_useful_words(embedding_model)\n",
    "\n",
    "embedding_size = embedding_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.use_cache_train_data:\n",
    "\n",
    "    logger.info(\"step-2--->\" + u\"语料格式转换,加标注生成标准文件\" + \"--->START\")\n",
    "\n",
    "    raw_train_file = path_flatten(corpus_path)\n",
    "    create_label_data(trainPath, word_dict, raw_train_file)\n",
    "    logger.info(\"step-3--->\" + u\"按标点符号或是空格存储文件\" + \"--->START\")\n",
    "\n",
    "    documents_length = dataPreprocess.create_documents()\n",
    "else:\n",
    "    documents_length = args.documents_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"step-4--->\" + u\"对语料中的词统计排序生成索引\" + \"--->START\")\n",
    "\n",
    "lexicon, lexicon_reverse = dataPreprocess.create_lexicon(word_dict)\n",
    "\n",
    "logger.info(\"step-5--->\" + u\"对所有的词创建词向量\" + \"--->START\")\n",
    "\n",
    "useful_word_length, embedding_weights = dataPreprocess.create_embedding(\n",
    "    embedding_model, embedding_size, lexicon_reverse\n",
    ")\n",
    "\n",
    "logger.info(\"step-6--->\" + u\"生成标注以及索引\" + \"--->START\")\n",
    "\n",
    "label_2_index = dataPreprocess.create_label_index()\n",
    "\n",
    "label_2_index_length = len(label_2_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"step-7--->\" + u\"将语料中每一句和label进行索引编码\" + \"--->START\")\n",
    "if not args.use_cache_train_data:\n",
    "    dataPreprocess.create_matrix(lexicon, label_2_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there has bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"step-8--->\" + u\"将语料中每一句和label以最大长度统一长度,不足补零\" + \"--->START\")\n",
    "# if not args.max_len:\n",
    "max_len = dataPreprocess.maxlen_2d_list()\n",
    "#\n",
    "if not args.use_cache_train_data:\n",
    "    dataPreprocess.padding_sentences(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"step-9--->\" + u\"模型创建\" + \"--->START\")\n",
    "\n",
    "model = bilstm_cnn_crf(\n",
    "    max_len,\n",
    "    useful_word_length + 2,\n",
    "    label_2_index_length,\n",
    "    embedding_size,\n",
    "    embedding_weights,\n",
    ")\n",
    "logger.info(\"setp-9.1--->\" + \"加载模型\" + \"--->START\")\n",
    "if os.path.exists(trainPath.checkpoints_path):\n",
    "    model.load_weights(trainPath.checkpoints_path)\n",
    "logger.info(\"step-10--->\" + u\"模型训练\" + \"--->START\")\n",
    "\n",
    "if batch_size > documents_length:\n",
    "\n",
    "    logger.info(\"ERROR--->\" + u\"语料数据量过少，请再添加一些\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    trainPath.checkpoints_path,\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "_ = model.fit_generator(\n",
    "    generator=generate_batch(\n",
    "        trainPath=trainPath,\n",
    "        batch_size=batch_size, label_class=label_2_index_length\n",
    "    ),\n",
    "    steps_per_epoch=int(documents_length / batch_size),\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    workers=1,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "logger.info(\"step-11--->\" + u\"模型和字典保存\" + \"--->START\")\n",
    "\n",
    "model.save_weights(trainPath.weights_path)\n",
    "\n",
    "index_2_label = dataPreprocess.create_index_label()\n",
    "\n",
    "pickle.dump([lexicon, index_2_label], open(trainPath.lexicon_path, \"wb\"))\n",
    "\n",
    "pickle.dump(\n",
    "    [max_len, embedding_size, useful_word_length + 2, label_2_index_length],\n",
    "    open(trainPath.model_params_path, \"wb\"),\n",
    ")\n",
    "\n",
    "logger.info(\"step-12--->\" + u\"打印恢复模型的重要参数\" + \"--->START\")\n",
    "\n",
    "logger.info(\"sequence_max_length: \" + str(max_len))\n",
    "\n",
    "logger.info(\"embedding size: \" + str(embedding_size))\n",
    "\n",
    "logger.info(\"useful_word_length: \" + str(useful_word_length + 2))\n",
    "\n",
    "logger.info(\"label_2_index_length: \" + str(label_2_index_length))\n",
    "\n",
    "logger.info(u\"训练完成\" + \"--->OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
